# **Crawler**
#### Сенников Дмитрий, Плюснина Елена ФТ-203
---
### 1) **cache.py**
Класс для создания кэша посещённых и непосещённых ссылок

    load - загрузка

    save - сохранение

### 2) **parse.py**

    download_url - получение текста странички по url-ссылке

    HTML_parser - парсер, собирающий все внутренние ссылки странички

### 3) **crawler.py**
Класс для создания краулера 

    work - для всех непосещённых ссылок многопоточно выполняет _task и периодически обновляет кэши

    _task - выгружает текст, парсит в html и сохраняет его с помощью file_work.save_html

    _update_links - должен обновлять непосещённые ссылки

    _robot_parser - обработка robots.txt

### 4) **file_work.py**
Работа с внешними файлами

    save_html - сохранение html-страниц

    add_links_to_graph - сохранение ссылочного графа в json

    show_graph - вывод графа

### Запуск 

    python main.py -u https://wikipedia.org -rc 10 -th 1000 -fo "https://ru.wikipedia" -fe "https://ru.wikipedia.org/w/" -nr -c -g -H

Хороший ограниченный пример, который быстро закончится самостоятельно

    python main.py -u https://kadm.kmath.ru -fo "https://kadm.kmath.ru" -fe ".*/pages|.*/files"  -c -H

Помощь в консоли

    python main.py -h

Рекомендации

* Большинство сайтов требуют разрешения на перенаправление, а потому флаг -nr не рекомендуется в обычной работе

* Граф по флагу -g рисуется относительно последнего сохранённого графа. Поэтому, если хотите получить граф, сначала включите краулер на поиск нужных вам страниц, а затем после прерывания его работы запросите вывод графа

* Фильтры (-fo, -fe) пишутся в кавычках, чтобы можно было получить из них регулярное выражение
